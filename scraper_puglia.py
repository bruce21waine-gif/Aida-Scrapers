# -*- coding: utf-8 -*-
"""Codice Python per Scraper Regione Puglia (v5.6 - Ottimizzato)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yIO_m21UZBA-LLUCEuRICQ8p3IGoVE8J
"""

# -*- coding: utf-8 -*-

# --- Scraper per "Bonus Facile" - Versione 5.6 (Regione Puglia Ottimizzato) ---
# Obiettivo: Estrarre i bandi dal portale della Regione Puglia in modo rapido e affidabile.
# Correzione: Rimosso il passaggio di lettura del robots.txt che andava in timeout.
# Lo script ora utilizza direttamente l'URL della sitemap, come suggerito.
#
# Per eseguire questo script, assicurati di avere le librerie necessarie:
# py -m pip install requests beautifulsoup4 psycopg2-binary python-dotenv lxml

import os
import requests
from bs4 import BeautifulSoup
import psycopg2
from urllib.parse import urljoin
from dotenv import load_dotenv
import hashlib
import time
import random
import xml.etree.ElementTree as ET
import re

# Carica le variabili d'ambiente da un file .env
load_dotenv()

# --- CONFIGURAZIONE ---
DATABASE_URL = os.getenv("DATABASE_URL")

# --- STRATEGIA v5.6 ---
URL_TARGET_KEYWORDS = [
    'avviso-pubblico', 'bando', 'sostegno', 'contributo', 'incentivo', 'patto-di-cura', 'misura'
]

TITLE_INCLUSION_KEYWORDS = [
    'sostegno', 'contributo', 'bonus', 'incentivo', 'agevolazione', 'voucher',
    'reddito', 'famiglie', 'cittadini', 'disabilità', 'non autosufficienti',
    'giovani', 'studenti', 'cura', 'assistenza', 'sociale'
]

REGIONE_PUGLIA_BASE_URL = "https://www.regione.puglia.it"
# --- MODIFICA CHIAVE v5.6: URL Sitemap diretto ---
SITEMAP_URL = "https://www.regione.puglia.it/sitemap.xml"
ID_FONTE = "reg_puglia_principale"


def connect_to_db():
    """
    Crea e restituisce una connessione al database PostgreSQL con logica di retry.
    """
    attempts = 3
    for i in range(attempts):
        try:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                print("Errore: La variabile d'ambiente DATABASE_URL non è impostata.")
                return None

            if "DATABASE_URL=" in db_url:
                db_url = db_url.split("DATABASE_URL=")[1].strip('"\'')

            conn = psycopg2.connect(db_url)
            print("   -> Connessione al database riuscita.")
            return conn
        except psycopg2.OperationalError as e:
            print(f"   -> Tentativo {i+1}/{attempts} di connessione al database fallito.")
            print(f"      Dettagli: {e}")
            if i < attempts - 1:
                time.sleep(5)
            else:
                print("Errore: Impossibile connettersi al database dopo vari tentativi.")
                return None
    return None

def make_request_with_retry(url, timeout, attempts=3):
    """
    Esegue una richiesta HTTP con una logica di retry in caso di timeout.
    """
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
    for i in range(attempts):
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            response.raise_for_status()
            return response
        except (requests.exceptions.ConnectTimeout, requests.exceptions.ReadTimeout) as e:
            print(f"   -> Tentativo {i+1}/{attempts} fallito (Timeout). Riprovo tra 10 secondi...")
            if i < attempts - 1:
                time.sleep(10)
            else:
                print(f"   -> ERRORE: Timeout di connessione a {url} dopo {attempts} tentativi.")
                raise e
    return None


def get_urls_from_sitemap(sitemap_url):
    """
    Scarica e analizza una sitemap (o un sitemap index) per estrarre tutti gli URL finali.
    """
    print(f"1. Analisi della sitemap/index: {sitemap_url}")
    try:
        response = make_request_with_retry(sitemap_url, timeout=90)
        if not response: return []

        content = response.content
        root = ET.fromstring(content)
        namespace = '{http://www.sitemaps.org/schemas/sitemap/0.9}'

        if root.tag == f'{namespace}sitemapindex':
            print("   -> Rilevato Sitemap Index. Analizzo le sitemap individuali...")
            sitemap_links = [elem.text for elem in root.findall(f'{namespace}sitemap/{namespace}loc')]
            all_page_urls = []
            for link in sitemap_links:
                all_page_urls.extend(get_urls_from_sitemap(link))
            return all_page_urls
        elif root.tag == f'{namespace}urlset':
            urls = [elem.text for elem in root.findall(f'{namespace}url/{namespace}loc')]
            print(f"   -> Trovati {len(urls)} URL in questa sitemap.")
            return urls
        else:
            print("   -> Formato XML non riconosciuto.")
            return []

    except ET.ParseError as e:
        print(f"   -> ATTENZIONE: Impossibile analizzare la sitemap (file malformato). La ignoro. Errore: {e}")
        return []
    except Exception as e:
        print(f"   -> ERRORE durante l'analisi della sitemap: {e}")
        return []

def get_page_details(page_url):
    """
    Visita una pagina e ne estrae il titolo.
    """
    try:
        response = make_request_with_retry(page_url, timeout=45)
        if not response: return {"titolo": "Titolo non trovato (errore di rete)"}

        soup = BeautifulSoup(response.content, 'lxml')
        title_tag = soup.find('h1')
        if title_tag:
            return {"titolo": title_tag.get_text(strip=True)}
        return {"titolo": "Titolo non trovato"}
    except requests.exceptions.RequestException:
        return {"titolo": "Titolo non trovato (errore di rete)"}

def scrape_regione_puglia():
    """
    Funzione principale che esegue lo scraping e l'inserimento dei dati.
    """
    print("--- Avvio scraper per Regione Puglia (v5.6 - Ottimizzato) ---")
    print("Strategia: Sitemap Diretta + Filtro Positivo + Retry HTTP.")

    all_urls = get_urls_from_sitemap(SITEMAP_URL)
    if not all_urls:
        print("Scraper interrotto: nessun URL valido trovato in nessuna sitemap.")
        return

    print(f"\n   -> Totale URL raccolti da tutte le sitemap valide: {len(all_urls)}")

    print("\n2. Filtro di Livello 1 (su URL)...")
    candidate_urls = []
    for url in all_urls:
        if any(keyword in url for keyword in URL_TARGET_KEYWORDS):
            candidate_urls.append(url)

    if not candidate_urls:
        print("   -> Nessun URL candidato trovato nella sitemap.")
        return

    print(f"   -> Trovati {len(candidate_urls)} URL candidati.")
    print("\n3. Filtro di Livello 2 (Filtro Positivo su Titolo) e Inserimento...")

    conn = connect_to_db()
    if not conn:
        return

    cursor = conn.cursor()

    bandi_inseriti_totali = 0
    bandi_esistenti_totali = 0

    for link_ufficiale in candidate_urls:
        try:
            print(f"   -> Analizzando: {link_ufficiale}")
            details = get_page_details(link_ufficiale)
            titolo = details["titolo"]

            titolo_lower = titolo.lower()
            if not any(keyword in titolo_lower for keyword in TITLE_INCLUSION_KEYWORDS):
                print(f"      -> SCARTATO (titolo non contiene parole chiave positive): '{titolo}'")
                continue

            print(f"      -> PERTINENTE. Titolo: '{titolo}'")

            hash_id = hashlib.sha1(link_ufficiale.encode()).hexdigest()[:10]
            id_bando = f"reg_puglia_{hash_id}"

            bando_data = {
                "id_bando": id_bando, "titolo": titolo, "ente_erogatore": "Regione Puglia",
                "categoria": "Altro", "descrizione_breve": titolo, "link_ufficiale": link_ufficiale,
                "data_pubblicazione": None, "stato": "Aperto",
                "livello": "Regionale", "regione": "Puglia", "id_fonte": ID_FONTE
            }

            insert_query = (
                "INSERT INTO bandi (id_bando, titolo, ente_erogatore, categoria, descrizione_breve, link_ufficiale, data_pubblicazione, stato, livello, regione, id_fonte, data_ultimo_check) "
                "VALUES (%(id_bando)s, %(titolo)s, %(ente_erogatore)s, %(categoria)s, %(descrizione_breve)s, %(link_ufficiale)s, %(data_pubblicazione)s, %(stato)s, %(livello)s, %(regione)s, %(id_fonte)s, NOW()) "
                "ON CONFLICT (id_bando) DO UPDATE SET titolo = EXCLUDED.titolo, data_ultimo_check = NOW();"
            )

            cursor.execute(insert_query, bando_data)

            if cursor.rowcount > 0:
                print(f"         -> Inserito nel database.")
                bandi_inseriti_totali += 1
            else:
                print(f"         -> Già esistente nel database.")
                bandi_esistenti_totali += 1

        except Exception as e:
            print(f"   -> ERRORE durante l'elaborazione di '{link_ufficiale}': {e}")

        time.sleep(random.randint(1, 2))

    conn.commit()
    cursor.close()
    conn.close()

    print("\n--- Scraper per Regione Puglia completato ---")
    print(f"Bandi inseriti o aggiornati: {bandi_inseriti_totali}")
    print(f"Bandi già esistenti e non modificati: {bandi_esistenti_totali}")


if __name__ == "__main__":
    scrape_regione_puglia()