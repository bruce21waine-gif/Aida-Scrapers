# -*- coding: utf-8 -*-
"""Codice Python per Scraper Regione Puglia (v5.3 - Ottimizzato)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yx4UH1ykCxviHk5C1FSnhzE2fB3ktv8Z
"""

# -*- coding: utf-8 -*-

# --- Scraper per "Bonus Facile" - Versione 5.3 (Regione Puglia con Filtro Positivo) ---
# Obiettivo: Estrarre solo bandi di alta qualità dal portale della Regione Puglia.
# Correzione: Abbandonato il filtro a esclusione. Si passa a un filtro positivo
# (o a inclusione) sul titolo, che salva un bando solo se contiene parole
# chiave che indicano un beneficio diretto per i cittadini.
#
# Per eseguire questo script, assicurati di avere le librerie necessarie:
# py -m pip install requests beautifulsoup4 psycopg2-binary python-dotenv lxml

import os
import requests
from bs4 import BeautifulSoup
import psycopg2
from urllib.parse import urljoin
from dotenv import load_dotenv
import hashlib
import time
import random
import xml.etree.ElementTree as ET
import re

# Carica le variabili d'ambiente da un file .env
load_dotenv()

# --- CONFIGURAZIONE ---
DATABASE_URL = os.getenv("DATABASE_URL")

# --- STRATEGIA v5.3 ---
# Livello 1: Parole chiave da cercare negli URL per una prima scrematura
URL_TARGET_KEYWORDS = [
    'avviso-pubblico', 'bando', 'sostegno', 'contributo', 'incentivo', 'patto-di-cura', 'misura'
]

# Livello 2: Filtro Positivo. Un bando viene salvato SOLO SE il titolo contiene una di queste parole.
TITLE_INCLUSION_KEYWORDS = [
    'sostegno', 'contributo', 'bonus', 'incentivo', 'agevolazione', 'voucher',
    'reddito', 'famiglie', 'cittadini', 'disabilità', 'non autosufficienti',
    'giovani', 'studenti', 'cura', 'assistenza', 'sociale'
]

REGIONE_PUGLIA_BASE_URL = "https://www.regione.puglia.it"
ROBOTS_URL = urljoin(REGIONE_PUGLIA_BASE_URL, "robots.txt")
ID_FONTE = "reg_puglia_principale"


def connect_to_db():
    """
    Crea e restituisce una connessione al database PostgreSQL.
    """
    try:
        conn = psycopg2.connect(DATABASE_URL)
        return conn
    except psycopg2.OperationalError as e:
        print(f"Errore: Impossibile connettersi al database.")
        return None

def find_sitemap_url_from_robots(robots_url):
    """
    Legge il file robots.txt per trovare l'URL della sitemap.
    """
    print(f"1. Analisi di robots.txt: {robots_url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(robots_url, headers=headers, timeout=20)
        response.raise_for_status()

        match = re.search(r"Sitemap:\s*(https://\S+)", response.text, re.IGNORECASE)
        if match:
            sitemap_url = match.group(1).strip()
            print(f"   -> Trovato URL della sitemap: {sitemap_url}")
            return sitemap_url
        else:
            print("   -> ERRORE: Nessuna direttiva Sitemap trovata in robots.txt.")
            return None
    except requests.exceptions.RequestException as e:
        print(f"   -> ERRORE durante il download di robots.txt: {e}")
        return None

def get_urls_from_sitemap(sitemap_url):
    """
    Scarica e analizza una sitemap (o un sitemap index) per estrarre tutti gli URL finali.
    """
    print(f"2. Analisi della sitemap/index: {sitemap_url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(sitemap_url, headers=headers, timeout=30)
        response.raise_for_status()

        content = response.content
        root = ET.fromstring(content)
        namespace = '{http://www.sitemaps.org/schemas/sitemap/0.9}'

        if root.tag == f'{namespace}sitemapindex':
            print("   -> Rilevato Sitemap Index. Analizzo le sitemap individuali...")
            sitemap_links = [elem.text for elem in root.findall(f'{namespace}sitemap/{namespace}loc')]
            all_page_urls = []
            for link in sitemap_links:
                all_page_urls.extend(get_urls_from_sitemap(link))
            return all_page_urls
        elif root.tag == f'{namespace}urlset':
            urls = [elem.text for elem in root.findall(f'{namespace}url/{namespace}loc')]
            print(f"   -> Trovati {len(urls)} URL in questa sitemap.")
            return urls
        else:
            print("   -> Formato XML non riconosciuto.")
            return []

    except ET.ParseError as e:
        print(f"   -> ATTENZIONE: Impossibile analizzare la sitemap (file malformato). La ignoro. Errore: {e}")
        return []
    except Exception as e:
        print(f"   -> ERRORE durante l'analisi della sitemap: {e}")
        return []

def get_page_details(page_url):
    """
    Visita una pagina e ne estrae il titolo.
    """
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(page_url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml')
        title_tag = soup.find('h1')
        if title_tag:
            return {"titolo": title_tag.get_text(strip=True)}
        return {"titolo": "Titolo non trovato"}
    except requests.exceptions.RequestException:
        return {"titolo": "Titolo non trovato (errore di rete)"}

def scrape_regione_puglia():
    """
    Funzione principale che esegue lo scraping e l'inserimento dei dati.
    """
    print("--- Avvio scraper per Regione Puglia (v5.3 - Ottimizzato) ---")
    print("Strategia: Sitemap + Filtro Positivo sul Titolo.")

    sitemap_url = find_sitemap_url_from_robots(ROBOTS_URL)
    if not sitemap_url:
        print("Scraper interrotto.")
        return

    all_urls = get_urls_from_sitemap(sitemap_url)
    if not all_urls:
        print("Scraper interrotto.")
        return

    print(f"\n   -> Totale URL raccolti da tutte le sitemap valide: {len(all_urls)}")

    print("\n3. Filtro di Livello 1 (su URL)...")
    candidate_urls = []
    for url in all_urls:
        if any(keyword in url for keyword in URL_TARGET_KEYWORDS):
            candidate_urls.append(url)

    if not candidate_urls:
        print("   -> Nessun URL candidato trovato nella sitemap.")
        return

    print(f"   -> Trovati {len(candidate_urls)} URL candidati.")
    print("\n4. Filtro di Livello 2 (Filtro Positivo su Titolo) e Inserimento...")

    conn = connect_to_db()
    if not conn:
        return

    cursor = conn.cursor()

    bandi_inseriti_totali = 0
    bandi_esistenti_totali = 0

    for link_ufficiale in candidate_urls:
        try:
            print(f"   -> Analizzando: {link_ufficiale}")
            details = get_page_details(link_ufficiale)
            titolo = details["titolo"]

            # --- MODIFICA CHIAVE v5.3: Filtro Positivo ---
            titolo_lower = titolo.lower()
            if not any(keyword in titolo_lower for keyword in TITLE_INCLUSION_KEYWORDS):
                print(f"      -> SCARTATO (titolo non contiene parole chiave positive): '{titolo}'")
                continue

            print(f"      -> PERTINENTE. Titolo: '{titolo}'")

            hash_id = hashlib.sha1(link_ufficiale.encode()).hexdigest()[:10]
            id_bando = f"reg_puglia_{hash_id}"

            bando_data = {
                "id_bando": id_bando, "titolo": titolo, "ente_erogatore": "Regione Puglia",
                "categoria": "Altro", "descrizione_breve": titolo, "link_ufficiale": link_ufficiale,
                "data_pubblicazione": None, "stato": "Aperto",
                "livello": "Regionale", "regione": "Puglia", "id_fonte": ID_FONTE
            }

            insert_query = (
                "INSERT INTO bandi (id_bando, titolo, ente_erogatore, categoria, descrizione_breve, link_ufficiale, data_pubblicazione, stato, livello, regione, id_fonte, data_ultimo_check) "
                "VALUES (%(id_bando)s, %(titolo)s, %(ente_erogatore)s, %(categoria)s, %(descrizione_breve)s, %(link_ufficiale)s, %(data_pubblicazione)s, %(stato)s, %(livello)s, %(regione)s, %(id_fonte)s, NOW()) "
                "ON CONFLICT (id_bando) DO UPDATE SET titolo = EXCLUDED.titolo, data_ultimo_check = NOW();"
            )

            cursor.execute(insert_query, bando_data)

            if cursor.rowcount > 0:
                print(f"         -> Inserito nel database.")
                bandi_inseriti_totali += 1
            else:
                print(f"         -> Già esistente nel database.")
                bandi_esistenti_totali += 1

        except Exception as e:
            print(f"   -> ERRORE durante l'elaborazione di '{link_ufficiale}': {e}")

        time.sleep(random.randint(1, 2))

    conn.commit()
    cursor.close()
    conn.close()

    print("\n--- Scraper per Regione Puglia completato ---")
    print(f"Bandi inseriti o aggiornati: {bandi_inseriti_totali}")
    print(f"Bandi già esistenti e non modificati: {bandi_esistenti_totali}")


if __name__ == "__main__":
    scrape_regione_puglia()