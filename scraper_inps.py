# -*- coding: utf-8 -*-
"""Scraper INPS v5.6 (Connessione Robusta)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/187fdX0JbMbyHb0jqXdKrl2fI21VvOqJZ
"""

# -*- coding: utf-8 -*-

# --- Scraper per "Bonus Facile" - Versione 5.6 (Connessione Robusta) ---
# Obiettivo: Estrarre i dettagli (ISEE, età) dalle pagine dei bonus.
# Correzione: Aggiunta una logica di retry alla connessione del database
# per gestire errori di rete temporanei come "Network is unreachable".
#
# Per eseguire questo script, assicurati di avere le librerie necessarie:
# py -m pip install requests beautifulsoup4 psycopg2-binary python-dotenv lxml

import os
import requests
from bs4 import BeautifulSoup
import psycopg2
from urllib.parse import urljoin
from dotenv import load_dotenv
import hashlib
import time
import random
import xml.etree.ElementTree as ET
import re

# Carica le variabili d'ambiente da un file .env
load_dotenv()

# --- CONFIGURAZIONE ---
DATABASE_URL = os.getenv("DATABASE_URL")

URL_TARGET_KEYWORDS = {
    "bonus-asilo-nido": "Bonus Asilo Nido",
    "contributo-per-sostenere-le-spese-relative-a-sessioni-di-psicoterapia-bonus-psicologo": "Bonus Psicologo",
    "assegno-unico-e-universale-per-i-figli-a-carico": "Assegno Unico e Universale",
    "supporto-per-la-formazione-e-il-lavoro": "Supporto Formazione e Lavoro",
    "assegno-di-inclusione": "Assegno di Inclusione (ADI)",
    "carta-cultura-e-carta-del-merito": "Carta Cultura e del Merito",
    "nuova-assicurazione-sociale-per-l-impiego-naspi": "Indennità NASpI",
    "decontribuzione-per-le-lavoratrici-con-figli": "Bonus Mamme Lavoratrici",
    "assegno-sociale": "Assegno Sociale"
}

TITLE_EXCLUSION_KEYWORDS = [
    'video guida', 'requisiti', 'compatibilità', 'comunicazioni',
    'come funziona', 'faq', 'come accedere', 'cosa rientra', 'obblighi',
    'maggiorazione', 'sospensione', 'revoca', 'decadenza', 'consultazione',
    'domanda', 'servizio', 'visualizzazione', 'simulazione', 'dichiarazioni'
]

INPS_BASE_URL = "https://www.inps.it"
ROBOTS_URL = urljoin(INPS_BASE_URL, "robots.txt")
ID_FONTE = "cat_inps"


def connect_to_db():
    """
    Crea e restituisce una connessione al database PostgreSQL con logica di retry.
    """
    # --- MODIFICA CHIAVE v5.6: Logica di Retry ---
    attempts = 3
    for i in range(attempts):
        try:
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                print("Errore: La variabile d'ambiente DATABASE_URL non è impostata.")
                return None

            if "DATABASE_URL=" in db_url:
                db_url = db_url.split("DATABASE_URL=")[1].strip('"\'')

            conn = psycopg2.connect(db_url)
            print("   -> Connessione al database riuscita.")
            return conn
        except psycopg2.OperationalError as e:
            print(f"   -> Tentativo {i+1}/{attempts} di connessione al database fallito.")
            print(f"      Dettagli: {e}")
            if i < attempts - 1:
                time.sleep(5) # Attendi 5 secondi prima di riprovare
            else:
                print("Errore: Impossibile connettersi al database dopo vari tentativi.")
                return None
    return None


def find_sitemap_url_from_robots(robots_url):
    """
    Legge il file robots.txt per trovare l'URL della sitemap.
    """
    print(f"1. Analisi di robots.txt: {robots_url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(robots_url, headers=headers, timeout=20)
        response.raise_for_status()

        match = re.search(r"Sitemap:\s*(https://\S+)", response.text, re.IGNORECASE)
        if match:
            sitemap_url = match.group(1).strip()
            print(f"   -> Trovato URL della sitemap: {sitemap_url}")
            return sitemap_url
        else:
            print("   -> ERRORE: Nessuna direttiva Sitemap trovata in robots.txt.")
            return None
    except requests.exceptions.RequestException as e:
        print(f"   -> ERRORE durante il download di robots.txt: {e}")
        return None

def get_urls_from_sitemap(sitemap_url):
    """
    Scarica e analizza la sitemap per estrarre tutti gli URL.
    """
    print(f"2. Analisi della sitemap: {sitemap_url}")
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(sitemap_url, headers=headers, timeout=30)
        response.raise_for_status()

        root = ET.fromstring(response.content)
        urls = [elem.text for elem in root.findall('{http://www.sitemaps.org/schemas/sitemap/0.9}url/{http://www.sitemaps.org/schemas/sitemap/0.9}loc')]

        print(f"   -> Trovati {len(urls)} URL totali nella sitemap.")
        return urls

    except Exception as e:
        print(f"   -> ERRORE durante l'analisi della sitemap: {e}")
        return []

def get_page_details(page_url):
    """
    Visita una pagina e tenta di estrarre titolo, ISEE, età e scadenza.
    """
    print(f"      -> Visitando la pagina per estrarre dettagli...")
    details = {"titolo": "Titolo non trovato", "isee_max": None, "eta_min": None, "eta_max": None}
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(page_url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'lxml')

        title_tag = soup.find('h1')
        if title_tag:
            details["titolo"] = title_tag.get_text(strip=True)

        page_text = soup.get_text().lower()

        isee_match = re.search(r'isee (?:non superiore a|di|fino a|massimo di|pari o inferiore a) ([\d\.,]+)', page_text)
        if isee_match:
            isee_str = isee_match.group(1).replace('.', '').replace(',', '.')
            details["isee_max"] = float(isee_str)
            print(f"         -> Trovato ISEE massimo: {details['isee_max']}")

        eta_match = re.search(r'età compresa tra ([\d]+) e ([\d]+) anni', page_text)
        if eta_match:
            details["eta_min"] = int(eta_match.group(1))
            details["eta_max"] = int(eta_match.group(2))
            print(f"         -> Trovato range di età: {details['eta_min']} - {details['eta_max']}")
        else:
            eta_max_match = re.search(r'(?:fino a|non superiore a|minore di) ([\d]+) anni', page_text)
            if eta_max_match:
                details["eta_max"] = int(eta_max_match.group(1))
                print(f"         -> Trovata età massima: {details['eta_max']}")

        return details

    except Exception as e:
        print(f"         -> Errore durante l'estrazione dei dettagli: {e}")
        return details

def select_best_url(url_list):
    """
    Da una lista di URL, seleziona quello più corto, che è solitamente la pagina principale.
    """
    if not url_list:
        return None
    return min(url_list, key=len)

def scrape_inps_bandi():
    """
    Funzione principale che esegue lo scraping e l'inserimento dei dati.
    """
    print("--- Avvio scraper per INPS (v5.6) ---")
    print("Strategia: Sitemap + De-duplicazione + Estrazione Dettagli.")

    sitemap_url = find_sitemap_url_from_robots(ROBOTS_URL)
    if not sitemap_url:
        print("Scraper interrotto.")
        return

    all_urls = get_urls_from_sitemap(sitemap_url)
    if not all_urls:
        print("Scraper interrotto.")
        return

    print("\n3. Filtro di Livello 1 (su URL) e Raggruppamento...")

    grouped_urls = {key: [] for key in URL_TARGET_KEYWORDS}
    for url in all_urls:
        for keyword in URL_TARGET_KEYWORDS:
            if keyword in url:
                grouped_urls[keyword].append(url)
                break

    print("\n4. De-duplicazione Intelligente...")
    best_urls = {}
    for keyword, url_list in grouped_urls.items():
        if url_list:
            best = select_best_url(url_list)
            print(f"   -> Gruppo '{URL_TARGET_KEYWORDS[keyword]}': {len(url_list)} candidati. Scelto: {best}")
            best_urls[keyword] = best

    if not best_urls:
        print("   -> Nessun URL candidato trovato.")
        return

    print(f"\n   -> Trovati {len(best_urls)} URL principali.")
    print("\n5. Filtro di Livello 2 (su Titolo) e Inserimento...")

    conn = connect_to_db()
    if not conn:
        return

    cursor = conn.cursor()

    bandi_inseriti_totali = 0
    bandi_esistenti_totali = 0

    for keyword, link_ufficiale in best_urls.items():
        try:
            print(f"   -> Analizzando: {link_ufficiale}")
            details = get_page_details(link_ufficiale)
            titolo = details["titolo"]

            titolo_lower = titolo.lower()
            if any(word in titolo_lower for word in TITLE_EXCLUSION_KEYWORDS):
                print(f"      -> SCARTATO (titolo non pertinente): '{titolo}'")
                continue

            print(f"      -> PERTINENTE. Titolo: '{titolo}'")

            hash_id = hashlib.sha1(link_ufficiale.encode()).hexdigest()[:10]
            id_bando = f"naz_inps_nodate_{hash_id}"

            bando_data = {
                "id_bando": id_bando, "titolo": titolo, "ente_erogatore": "INPS",
                "categoria": "Altro", "descrizione_breve": titolo, "link_ufficiale": link_ufficiale,
                "data_pubblicazione": None, "stato": "Aperto",
                "livello": "Nazionale", "id_fonte": ID_FONTE,
                "isee_max": details["isee_max"],
                "eta_min": details["eta_min"],
                "eta_max": details["eta_max"],
            }

            insert_query = (
                "INSERT INTO bandi (id_bando, titolo, ente_erogatore, categoria, descrizione_breve, link_ufficiale, data_pubblicazione, stato, livello, id_fonte, isee_max, eta_min, eta_max, data_ultimo_check) "
                "VALUES (%(id_bando)s, %(titolo)s, %(ente_erogatore)s, %(categoria)s, %(descrizione_breve)s, %(link_ufficiale)s, %(data_pubblicazione)s, %(stato)s, %(livello)s, %(id_fonte)s, %(isee_max)s, %(eta_min)s, %(eta_max)s, NOW()) "
                "ON CONFLICT (id_bando) DO UPDATE SET "
                "titolo = EXCLUDED.titolo, isee_max = EXCLUDED.isee_max, eta_min = EXCLUDED.eta_min, eta_max = EXCLUDED.eta_max, data_ultimo_check = NOW();"
            )

            cursor.execute(insert_query, bando_data)

            if cursor.rowcount > 0:
                print(f"         -> Inserito nel database.")
                bandi_inseriti_totali += 1
            else:
                print(f"         -> Già esistente nel database.")
                bandi_esistenti_totali += 1

        except Exception as e:
            print(f"   -> ERRORE durante l'elaborazione di '{link_ufficiale}': {e}")

        time.sleep(random.randint(1, 2))

    conn.commit()
    cursor.close()
    conn.close()

    print("\n--- Scraper per INPS completato ---")
    print(f"Bandi inseriti o aggiornati: {bandi_inseriti_totali}")
    print(f"Bandi già esistenti e non modificati: {bandi_esistenti_totali}")


if __name__ == "__main__":
    scrape_inps_bandi()